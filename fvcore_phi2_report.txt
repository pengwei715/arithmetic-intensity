PhiForCausalLM(
  (model): PhiModel(
    (embed_tokens): Embedding(51200, 2560)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-31): 32 x PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)
)
PhiForCausalLM(
  #params: 2.78G, #flops: 2.65G
  (model): PhiModel(
    #params: 2.65G, #flops: 2.52G
    (embed_tokens): Embedding(
      51200, 2560
      #params: 0.13G, #flops: 0
    )
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      #params: 2.52G, #flops: 2.52G
      (0): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (1): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (2): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (3): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (4): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (5): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (6): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (7): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (8): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (9): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (10): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (11): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (12): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (13): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (14): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (15): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (16): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (17): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (18): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (19): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (20): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (21): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (22): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (23): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (24): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (25): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (26): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (27): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (28): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (29): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (30): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (31): PhiDecoderLayer(
        #params: 78.67M, #flops: 78.66M
        (self_attn): PhiAttention(
          #params: 26.22M, #flops: 26.22M
          (q_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (k_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (v_proj): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (dense): Linear(
            in_features=2560, out_features=2560, bias=True
            #params: 6.56M, #flops: 6.55M
          )
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          #params: 52.44M, #flops: 52.43M
          (activation_fn): NewGELUActivation()
          (fc1): Linear(
            in_features=2560, out_features=10240, bias=True
            #params: 26.22M, #flops: 26.21M
          )
          (fc2): Linear(
            in_features=10240, out_features=2560, bias=True
            #params: 26.22M, #flops: 26.21M
          )
        )
        (input_layernorm): LayerNorm(
          (2560,), eps=1e-05, elementwise_affine=True
          #params: 5.12K, #flops: 12.8K
        )
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm(
      (2560,), eps=1e-05, elementwise_affine=True
      #params: 5.12K, #flops: 12.8K
    )
  )
  (lm_head): Linear(
    in_features=2560, out_features=51200, bias=True
    #params: 0.13G, #flops: 0.13G
  )
)
| module                          | #parameters or shape   | #flops     |
|:--------------------------------|:-----------------------|:-----------|
| model                           | 2.78G                  | 2.648G     |
|  model                          |  2.649G                |  2.517G    |
|   model.embed_tokens            |   0.131G               |   0        |
|    model.embed_tokens.weight    |    (51200, 2560)       |            |
|   model.layers                  |   2.517G               |   2.517G   |
|    model.layers.0               |    78.671M             |    78.661M |
|    model.layers.1               |    78.671M             |    78.661M |
|    model.layers.2               |    78.671M             |    78.661M |
|    model.layers.3               |    78.671M             |    78.661M |
|    model.layers.4               |    78.671M             |    78.661M |
|    model.layers.5               |    78.671M             |    78.661M |
|    model.layers.6               |    78.671M             |    78.661M |
|    model.layers.7               |    78.671M             |    78.661M |
|    model.layers.8               |    78.671M             |    78.661M |
|    model.layers.9               |    78.671M             |    78.661M |
|    model.layers.10              |    78.671M             |    78.661M |
|    model.layers.11              |    78.671M             |    78.661M |
|    model.layers.12              |    78.671M             |    78.661M |
|    model.layers.13              |    78.671M             |    78.661M |
|    model.layers.14              |    78.671M             |    78.661M |
|    model.layers.15              |    78.671M             |    78.661M |
|    model.layers.16              |    78.671M             |    78.661M |
|    model.layers.17              |    78.671M             |    78.661M |
|    model.layers.18              |    78.671M             |    78.661M |
|    model.layers.19              |    78.671M             |    78.661M |
|    model.layers.20              |    78.671M             |    78.661M |
|    model.layers.21              |    78.671M             |    78.661M |
|    model.layers.22              |    78.671M             |    78.661M |
|    model.layers.23              |    78.671M             |    78.661M |
|    model.layers.24              |    78.671M             |    78.661M |
|    model.layers.25              |    78.671M             |    78.661M |
|    model.layers.26              |    78.671M             |    78.661M |
|    model.layers.27              |    78.671M             |    78.661M |
|    model.layers.28              |    78.671M             |    78.661M |
|    model.layers.29              |    78.671M             |    78.661M |
|    model.layers.30              |    78.671M             |    78.661M |
|    model.layers.31              |    78.671M             |    78.661M |
|   model.final_layernorm         |   5.12K                |   12.8K    |
|    model.final_layernorm.weight |    (2560,)             |            |
|    model.final_layernorm.bias   |    (2560,)             |            |
|  lm_head                        |  0.131G                |  0.131G    |
|   lm_head.weight                |   (51200, 2560)        |            |
|   lm_head.bias                  |   (51200,)             |            |
